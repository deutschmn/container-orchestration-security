%% example text content
%% scrartcl and scrreprt starts with section, subsection, subsubsection, ...
%% scrbook starts with part (optional), chapter, section, ...
\chapter{Kubernetes Cluster Security}\label{chap:clusterSecurity}

In Kubernetes there are many aspects to consider when looking at it from a security perspective. With its flexibility also comes considerable complexity. In order to come by it and discuss it in a structured manner, we have created the model depicted in Figure~\ref{fig:secmodel}. It is aimed at giving a holistic view over all aspects that might influence the security of a Kubernetes cluster and uses structure ideas from~\textcite{securingkubernetes} and~\textcite{kubernetessecurity}.

\myfig{layers.pdf} %% filename in figures folder
  {width=1.0\textwidth,height=0.6\textheight}%% maximum width/height, aspect ratio will be kept
  {This security model shows the different layers that have to be considered when holistically regarding Kubernetes security.}%% caption
  {Security Model}%% optional (short) caption for table of figures
  {fig:secmodel}%% label

The model is composed of the following four layers:

\begin{enumerate}
    \item The lowest layer we call \textbf{Base Infrastructure Security} and it concerns all components on which Kubernetes itself builds. These include the Operating System, the container engine (most likely Docker), and the infrastructure of the public or private cloud provider when for example using Google Cloud Platform or Amazon Web Services as an \ac{IaaS} provider. This layer is described in Section~\ref{sec:layer1}.
    \item Upon that builds \textbf{Kubernetes Infrastructure Security} which is described in detail in Section~\ref{sec:layer2}. It concerns everything related to Kubernetes' control plane components and their configuration in terms of security. Potential issues there include abuse of the internal Kubernetes APIs, such as the Kubelet API, or intercepting control plane traffic.
    \item Additionally to the configuration of the Kubernetes control plane components themselves, there are also security components provided by Kubernetes to secure clusters. These components include Kubernetes' authentication and authorisation mechanisms, pod security policies, secrets management and many more. We group them together in the layer \textbf{Kubernetes Security Controls} as described in detail in Section~\ref{sec:layer3}.
    \item The top-most layer is called \textbf{App and Container Security} and makes use of all layers underneath it. On this layer, the actual applications run in containers, which in turn are located in pods. Exploiting of vulnerabilities in the application code might breach this layer. These topics are described in detail in Section~\ref{sec:layer4}.
\end{enumerate}

The layered architecture also makes sense when considering damage control aspects of Kubernetes security: As one layer on the top breaches, the lower layers can still prevent further damage. For example, when there is a vulnerability in the application code on layer 4, a properly configured set of permissions on layer 3 for the pod might still prevent further damage. Inversely, an insecure port left open in a control plane component, such as the API server, on layer 2 might allow an attacker to infiltrate a whole cluster, taking over all pods, containers and applications on the upper layers. 
	
\section{Base Infrastructure Security} \label{sec:layer1}

The base infrastructure in which a Kubernetes cluster is run sets the foundation for the whole cluster's security, as explained in~\textcite{securingkubernetesBaseInfra}. However well-tuned the Kubernetes cluster configuration is, a breach in the underlying infrastructure of a Kubernetes node can compromise the whole cluster. Depending on whether a cluster runs in a public or private cloud environment, some of the following guidelines may apply. When a cluster runs in a managed environment, such as on Google Kubernetes Engine (GKE) or Amazon Elastic Container Service for Kubernetes (Amazon EKS), a lot of this configuration may already be taken care of by the provider.

\paragraph{Operating System}

Every component in Kubernetes is run by an underlying operating system, mostly a flavour of Linux. While vulnerabilities in the OS itself are outside the scope of this thesis, some general guidelines should be followed to minimise the risk of a breach on OS level:

\begin{itemize}
    \item An operating system with container support that has a minimal attack surface should be used, such as \textit{CoreOS Container Linux}\footnote{\url{https://coreos.com/os/docs/latest/}, accessed 2019-06-10} or \textit{k3OS}\footnote{\url{https://k3os.io}, accessed 2019-06-10} that has been specifically designed for Kubernetes.
    \item Just like Kubernetes should be regularly patched, the operating system kernel should also be running at the most recent version at all times.
    \item For many Linux distributions there are hardening guides that can be used as a reference for the system configuration, such as the \textit{CoreOS Container Linux hardening guide}\footnote{\url{https://coreos.com/os/docs/latest/hardening-guide.html}, accessed 2019-06-10}. 
\end{itemize}

\paragraph{Container Runtime}

Upon the OS builds the container runtime in which all containers are run in the cluster. At the time of this writing, mostly the runtime is \textit{Docker}\footnote{\url{https://www.docker.com}, accessed 2019-06-10}, but also alternatives that comply with the \textit{Kubernetes Container Runtime Interface}\footnote{\url{https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/}, accessed 2019-06-10} such as \textit{containerd}\footnote{\url{https://containerd.io}, accessed 2019-06-10} gain traction.

Besides the OS, also the container runtime should always be up-to-date and configured securely. It is essential not to forget about it in the stack and acknowledge that containers are not virtual machines and do not provide the same kind of isolation. A breach out of a container can, therefore, mean that not only a pod, but an entire node can be compromised.

\paragraph{Network environment}

Outside access to cluster resources should be as restricted as possible. In most configurations, access to functionality provided by the cluster is exposed only via Kubernetes Services. Hence it is neither necessary nor recommended for every node to be exposed to the public internet. The attack surface of the whole cluster can, therefore, be reduced by putting all nodes in a private network and exposing only the services via a load balancer.

\paragraph{Trusted Platform Modules}

\acp{TPM}, as standardised in~\textcite{TPMStandard}, can be a powerful addition to the base infrastructure to increase its security. As demonstrated by~\textcite{TPM}, \acp{TPM} can be used to store cryptographic keys that stay secure from an attacker even if an entire node is compromised. 

\section{Kubernetes Infrastructure Security} \label{sec:layer2}

The security of Kubernetes infrastructure concerns the configuration of all components of the Kubernetes control plane and their internal communications. When setting up and administering a cluster, there are many configurations that, at first sight, do not appear to be critical to security. Still, the following aspects must not be neglected to ensure a secure cluster setup. Many best practice guidelines are listed in~\textcite{kubernetessecurity}.

\subsection{Cluster installers}

While it is possible to set up a cluster completely manually\footnote{\url{https://github.com/kelseyhightower/kubernetes-the-hard-way}, accessed 2019-06-11}, most are set up using cluster installers such as \mycode{kubeadm}\footnote{\url{https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/}, accessed 2019-06-11} or \mycode{kops}\footnote{\url{https://github.com/kubernetes/kops}, accessed 2019-06-11}. They can be used to bootstrap all control plane components, set up the \ac{PKI} and network configuration, and join worker nodes to the cluster.

Even though most installers come with sane default configurations, it is crucial to verify them and check what is done under the hood, as emphasised by~\textcite{securingkubernetesConfK8SClusterComponents}. 

\subsection{API server}

The API server is an essential control plane component, as an attacker that gains control over it has the equivalent of root access to the whole cluster, as described by~\cite{kubernetessecurity}. The critical aspect of how the API itself is secured and how the API server fulfils its role as the cluster's \ac{CA} is described in Section \ref{sec:apiAccessControl}, as this resembles a Kubernetes security control and is therefore grouped in the corresponding layer of our model. 

Still, any misconfiguration of the API server might mean leaving the front door to the cluster open, which is why its configuration should always be reviewed from a security perspective. This includes the fact that the Kubernetes API should generally not be exposed to the public internet if there is no pressing need.

\subsection{Kubelets}

A Kubelet is an agent running on every node that is responsible for running its containers, reporting metrics and similar. It receives its commands from the control plane. For that, it provides the so-called \textit{Kubelet API}, which is undocumented, as it is not supposed to be used by anyone other than the control plane. 

This API needs to be adequately protected, as otherwise it can be misused as shown by~\textcite{kubeletBackdoor}. This misconfiguration gave any attacker with network access to nodes an unauthenticated API backdoor to the cluster. In the Kubernetes version 1.15, which is the most current one at the time of this writing, the API is protected by default. The insecure port is closed and the secure ones calls back to the API server to verify whether a request should be allowed (see \textit{Node Authorisation} in Section \ref{ssec:authorisation}). 

It is worth noting that disallowing node network access, as recommended in \ref{sec:layer1}, could have prevented this exploit in the first place.

\subsection{etcd}\label{ssec:etcd}

\mycode{etcd} is the central storage location in Kubernetes that stores all cluster data in a key-value data structure. The API server is the only component that is supposed to access it directly.

\paragraph{Access restriction}
In order to enforce that only the API server can communicate with \mycode{etcd}, network policies, as later described in Section~\ref{ssec:networking}, should set up and an adequate certificate configuration\footnote{\url{https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/security.md}, accessed 2019-06-11} should be put in place.

\paragraph{Encryption}
By default, all data in \mycode{etcd} is stored in plain text. As it also stores all secrets, sensitive information might be exposed, if the data on the volume used by \mycode{etcd} leaks. In order to mitigate that, encryption can and should be enabled by providing the API server with an encryption configuration using the startup parameter \mycode{--encryption-provider-config} as described in the \textcite{k8sdocs}\footnote{\url{https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/}, accessed 2019-06-11}.


\subsection{Kubernetes Dashboard}

The Dashboard\footnote{\url{https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/}, accessed 2019-07-19} is a Web UI for administrating a Kubernetes cluster and gives a convenient overview over the pods, deployments, services and other objects currently deployed. However, as the recent incident at Tesla, described in~\textcite{teslaLeak}, illustrates, leaving the Dashboard exposed to the public internet or leaving it configured insecurely can compromise the security of an entire cluster. Therefore~\textcite{kubernetessecurity} suggest to follow these guidelines to secure it:

\begin{itemize}
    \item Only authenticated users should be allowed access.
    \item The service account the Dashboard is using should have limited privileges so that users cannot misuse its permissions and rather log in with their own users.
    \item The dashboard should not be exposed to the public internet.
\end{itemize}

\section{Kubernetes Security Controls} \label{sec:layer3}

Kubernetes provides cluster operates with an extensive set of options and tools to tweak the security of a cluster. These build upon the Kubernetes and base infrastructure and are very important to configure correctly. 

\subsection{Namespaces} \label{sec:namespaces}

As described in the documentation~\textcite{k8sdocs}\footnote{\url{https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/}, accessed 2019-05-31}, almost every resource in a Kubernetes cluster belongs to a namespace. Exceptions include only namespaces themselves and some low-level resources such as nodes. 

All resources belonging to the control plane components reside within the \mycode{kube-system} namespace, while by default all other objects are located in the \mycode{default} namespace. 

Generally, namespaces can be used to avoid naming conflicts, but also to allow for finer grained access control. For example, using API access control, a role can be created so that a user can list all pods from a particular namespace, but not for any other namespace, as further described in Section~\ref{sec:apiAccessControl}. Using this feature makes sense primarily when the cluster is used by a large number of users from, for example, different projects or departments. 

Namespaces can sometimes alleviate the need for creating separate clusters to isolate components from each other. Some use cases might include different namespaces per team, app or environment (such as different namespaces for development, testing and production). 

However, as pointed out in~\textcite{namespacesInsights}, while namespaces are suited well for logical partitioning and assigning permission sets on API access level, they do not enforce the partitioning by setting firewall rules or such. Any cluster user or resource can access any other resource in the cluster, even when they are in different namespaces. To achieve partitioning on the network level, see Section~\ref{ssec:networking}.

\subsection{API Access Control} \label{sec:apiAccessControl}

As explained in Chapter \ref{cha:background}, the only component in a Kubernetes cluster that is allowed to modify the cluster state in etcd directly is the API server. Therefore all requests that involve reading or modifying the cluster state must be performed via the Kubernetes API. It is well-documented in~\textcite{k8sdocsApi} and is very powerful, as it is also used by all control plane components to communicate with each other. Hence it needs to be carefully protected from unauthorised access. The API server achieves that using three steps to very if and how a request should be performed, as depicted in Figure~\ref{fig:apiServerAccessControl}.

\myfig{api_access_control_itnext.png}{width=0.8\textwidth}{This image from~\textcite{securingkubernetesConfK8SClusterComponents} displays the steps performed by the API server before it persists a request made to it.}{API server access control flow}{fig:apiServerAccessControl}

First, \textbf{Authentication} is performed to determine the identity of the user or system that is trying to access the API. After the identity of the accessing party has been determined, \textbf{Authorisation} is used to decide whether they are allowed to access or modify the requested resource. Finally \textbf{Admission Controllers} are applied to the request to validate or mutate it, before it is ultimately persisted.

\subsubsection{Authentication} \label{ssec:authentication}

Users in Kubernetes can be either Service Accounts or normal (human) users. 

Service Accounts are managed directly by Kubernetes and give identities to apps running inside pods as described in~\textcite{k8sdocs}\footnote{\url{https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/}, accessed 2019-06-10}. With these identities, the applications can access the API server. Service Accounts are Kubernetes objects and are stored in \mycode{etcd}. A pod can be configured to use a particular service account, which results in the corresponding certificates being mounted into the pod's file system. With these, the application running in it can authenticate with the API server. 

For human users, no Kubernetes objects in \mycode{etcd} exist. The administration of those is outsourced to an external entity. The API server offers multiple strategies to authenticate such users, a subset of which are the following:

\begin{itemize}
    \item The default way of authentication between control plane components is using \textbf{X.509 certificates} within the \ac{PKI} of the cluster. For that to function, the API server is handed the \ac{CA} file using the start-up parameter \mycode{--client-ca-file= /path/to/ca.crt}. Users can then authenticate using a certificate signed by the provided CA. 
    
    At the time of this writing, however, Kubernetes does not have support for \ac{CRL} or \ac{OCSP}, with which certificates could be revoked. The issue is currently under debate within the Kubernetes developer community\footnote{\url{https://github.com/kubernetes/kubernetes/issues/18982}, accessed 2019-05-28}. In the meantime, authorisation has to be used to strip users from permissions retroactively.

    \item An \textbf{Authentication Proxy} can be used to outsource the task of authentication to an external entity that might run within our outside of the cluster.
    
    \item In order to enable support for \ac{SSO}, the API server can be configured to use \textbf{\ac{OIDC}}\footnote{\url{https://openid.net/connect/}, accessed 2019-05-28} for authentication. It builds upon the OAuth 2.0 flow and allows the use of external identity providers that implement the standard, such as the well-known \textit{"Sign in with Facebook/Google/Microsoft"} known from various places on the web. 

   \item For testing and demo purposes it is also possible to use a \textbf{static password file} as per the definition in the RFC 7617 "The 'Basic' HTTP Authentication Scheme" by~\textcite{RFC7617}. However, it is rarely practical in real-world use cases as the password file needs to be manually maintained.
	
\end{itemize}

Authentication considers no actual permissions. It only determines the identity of the authenticating entity and several attributes about it, such as the username and group memberships. This information is passed on to the next step, authorisation, in which the actual mapping step of who is allowed to do what happens. 

\subsubsection{Authorisation} \label{ssec:authorisation}

As is the case for authentication, Kubernetes also provides several options for how authorisation can be performed as defined in the documentation~\textcite{k8sdocs}\footnote{\url{https://kubernetes.io/docs/reference/access-authn-authz/authorization/}, accessed 2019-05-31}:

\begin{itemize}
    \item \textbf{Node Authorisation} is used only by \mycode{kubelet}s, so that nodes are equipped with the minimum set of permissions they need to operate within the cluster.
    \item \textbf{\ac{ABAC}} grants permissions by explicit policies based on the users' attributes. \ac{ABAC}, however, has been deprecated since Kubernetes version 1.6 and is not recommended to be used anymore.
    \item \textbf{\ac{RBAC}} defines roles that come with certain permissions. These roles can then be bound to users to grant them access.
    \item \textbf{Webhook Mode} queries an external REST service to outsource the authorisation to it.
\end{itemize}

As~\textcite{ABACvsRBAC} point out, conceptually there are advantages and disadvantages to both \ac{ABAC} and \ac{RBAC}. While \ac{ABAC} provides more flexibility, \ac{RBAC} lends itself better for analysis and risk assessment. The main advantage of \ac{ABAC} is that it can perform restrictions based on users' attributes, such as their locations. In the context of Kubernetes, however, its developers~\textcite{ABACvsRBACk8s} argue that only \ac{RBAC} should be used anymore. That is acceptable primarily as the third step of API access control in Kubernetes, \textit{Admission Control}, can provide filters to accommodate for such finer-grained control.

\ac{RBAC} is the recommended way of performing authorisation at the time of this writing, as it is easier to understand and configure than \ac{ABAC} while retaining most of its configuration power. Therefore, we will only consider it in this work and disregard the other options.

\paragraph{\ac{RBAC}}

\myfig{rbac.png}{width=0.5\textwidth}{This graphic from~\textcite{k8sCookBook} depicts the basic building blocks of \ac{RBAC}.}{RBAC building blocks}{fig:rbac}

The basic building blocks of \ac{RBAC} are depicted in Figure \ref{fig:rbac}. \textit{Roles} are used to define access to certain \textit{Resources}. Rules can either be defined cluster-wide or namespace wide, depending on the objects they control access to.

A role that allows reading pods is created by applying a YAML file like this: 

\begin{minted}[
    autogobble,
    frame=single,
    linenos
  ]{yaml}
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
\end{minted}

Besides some metadata, a role defines certain API groups (in the above example the \mycode{""} indicate the core API), resources and certain verbs that correspond to the actions that are allowed on the resource. 

In order to assign the role to an \textit{Entity}, in this example case a \mycode{ServiceAccount} called \mycode{example-app-sa}, a \textit{RoleBinding} for the namespace \mycode{default} can be created by applying a YAML file like this:

\begin{minted}[
    autogobble,
    frame=single,
    linenos
  ]{yaml}
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: example-app-sa 
  namespace: default
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
\end{minted}

Besides \textit{Roles} and \textit{RoleBindings}, there are also \textit{ClusterRoles} and \textit{ClusterRoleBindings} that work similarly, just that they manage permissions on the scope of the whole cluster instead of being bound to a specific namespace.

As for all security configurations, the principle of Least Privilege applies especially when it comes to the configuration of \ac{RBAC}. It has been initially defined by~\textcite{leastPrivilege} as \enquote{Every program and every privileged user of the system should operate using the least amount of privilege necessary to complete the job.}. 

Applied to Kubernetes API access control, this means that an application which does not need to access the Kubernetes API to function, as is likely the case for the vast majority of applications, should not be allowed to do so. As pointed out by \textcite{kubernetessecurity}, it is therefore recommended to disable the auto-mounting of the default Service Account into pods by setting \mycode{automountServiceAccountToken: false} in the pod specification. 

Section \ref{ssec:exaLayer3}. shows another practical example of an \ac{RBAC} configuration that adheres to this principle.

\subsubsection{Admission Control} \label{ssec:admissionControl}

As described in \cite{admissionControl} and \cite{k8sdocs}\footnote{\url{https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/}, accessed 2019-05-25}, admission control allows for a set of plug-ins to enforce rules about how a cluster is used, by being applied after a request to the API server has passed authentication and authorisation. These plug-ins can be either \textit{mutating}, \textit{validating} or both. First, all mutating admission controllers are executed, then the validating controllers. If any of them rejects the request, the processing is aborted and the request is rejected.

An especially important admission controller with respect to security is the built-in \mycode{PodSecurityPolicy}. It can be used to forbid containers running with privileged users or set containers' file systems to read-only. Both of these are sensible best practices, as further explained in the context of container security in Section \ref{ssec:exaLayer4}.

Besides standard admission controllers that ship with Kubernetes and should be disabled or altered only under certain circumstances, there is also support for custom controllers that build upon the standard \mycode{Mutating AdmissionWebhook} or \mycode{ValidatingAdmissionWebhook}. These allow for high flexibility as their admission is based on a REST call to a service running within the cluster. This feature can be used to develop custom restrictions, such as this example\footnote{\url{https://github.com/stackrox/admission-controller-webhook-demo}, accessed 2019-05-28} that causes all containers to be run with a non-root user, except they are explicitly marked to do otherwise. 

\subsection{Networking} \label{ssec:networking}

Communication between Kubernetes components and the outside world is a central aspect for Kubernetes to function. As described in detail in the article series \textcite{NetworkingExplained} and in the documentation \textcite{k8sdocs}\footnote{\url{https://kubernetes.io/docs/concepts/cluster-administration/networking/}, accessed 2019-06-10}, it can be broken down in to several aspects:

\begin{itemize}
    \item Containers within the same pod communicate with each other via localhost.
    \item Pods communicate with other pods and with the control plane adhering to the Kubernetes Networking model\footnote{\url{https://kubernetes.io/docs/concepts/cluster-administration/networking/\#the-kubernetes-network-model}, accessed 2019-06-10} that defines an overlay network for that purpose. The model is aimed to be very flexible and should allow for easy porting from virtual machines. It mainly builds upon the premise that pods can be addressed with their own IP addresses, despite multiple pods running on the same node. The network model is not directly implemented in the Kubernetes core, but rather different plugins can be used to meet its requirements. These plugins integrate with nodes and their kernels to establish the layer 3 (network layer) communication as defined in~\textcite{osiModel}.
    \item Communication with the outside world is handled by Services that can be bound to  Ingress\footnote{\url{https://kubernetes.io/docs/concepts/services-networking/ingress/}, accessed 2019-06-10} objects.
\end{itemize}

From a security perspective, an especially important topic in networking are Network Policies\footnote{\url{https://kubernetes.io/docs/concepts/services-networking/network-policies/}, accessed 2019-05-28}, which allow configuration on which pods are allowed to communicate with each other and with other network endpoints. Network policies are realised in the Kubernetes object \mycode{NetworkPolicy}. Still, not all networking plugins support them.

An example for a \mycode{NetworkPolicy} could look like this:

\begin{minted}[
    autogobble,
    frame=single,
    linenos
  ]{yaml}
  kind: NetworkPolicy
  apiVersion: networking.k8s.io/v1
  metadata:
    name: example-policy
  spec:
    podSelector:
      matchLabels:
        app: example-app
    policyTypes:
    - Egress
    - Ingress
    ingress:
    - from:
      - podSelector:
          matchLabels:
            access: "true"
\end{minted}

The \mycode{policyTypes} specify whether egress traffic, ingress traffic or both are affected by this policy. The white-list principle applies so that only the specified traffic is allowed. This example applies to all pods with the label \mycode{app} set to \mycode{example-app}. It allows incoming traffic only from pods with the label \mycode{access} set to \mycode{true} and prohibits all outgoing traffic.

In most cases, it makes sense to prohibit egress by default. In this case, even if a pod is entirely hijacked, it cannot directly communicate with the outside world via a reverse shell (connect-back shell). Of course, however, such a restriction could be bypassed by using a bind shell that listens on a specific port for an incoming connection from the attacker as explained in~\textcite{bindAndReverseShells}. Still, such an approach would require more effort from the attacker.

An example of how Network Policies can be used to secure a real-world cluster is given in Section \ref{ssec:exaLayer3}.

\section{App and Container Security} \label{sec:layer4}

The top-most layer of the model concerns the applications that run in the Kubernetes cluster within their container environments. Their security relies on the lower layers while also being protected by them.

\subsection{Application Security}

The largest attack surface on this layer naturally are the applications themselves. As the whole class of application vulnerabilities cannot be mitigated entirely, Kubernetes provides many damage control tools on lower levels that limit an attacker's possibilities, even when an application has been compromised. This is a good practice example of defence in depth, as explained in~\textcite{defenceInDepth}.

\paragraph{Image Scanning} 

Often, the containers that are run in a cluster build upon public images and included application packages. Image scanning tools, such as \textit{Anchore Engine}\footnote{\url{https://github.com/anchore/anchore-engine}, accessed 2019-06-28} and \textit{Clair}\footnote{\url{https://github.com/coreos/clair}, accessed 2019-06-28}, scan images before they are deployed and analyse whether they contain known vulnerabilities. Using the admission controller \mycode{ImagePolicyWebhook}, as explained in Section~\ref{ssec:admissionControl}, API access control can be configured to refuse the deployment of images for which vulnerabilities are reported.  

\subsection{Container Configuration}

In this section, several precautions are explained that can be taken on container level to increase cluster security.

\paragraph{Non-privileged containers}

As pointed out in detail in~\textcite{nonPrivContainers}, applications in containers should run with non-privileged users, as most of them should not require root permissions. This restriction can be enforced by adding the following statements to the \mycode{PodSecurityPolicy} configuration:

\begin{minted}[
    autogobble,
    frame=single,
    linenos
  ]{yaml}
allowPrivilegeEscalation: false
runAsUser:
  rule: 'MustRunAsNonRoot'
\end{minted}

One side effect of not running as root is that applications can not bind to well-known ports ($< 1024$), but this can be easily mitigated by using the port binding capabilities of the container runtime.

\paragraph{Minimal containers}

Containers should only run the bare minimum of applications they require to fulfil their aim. It is therefore considered bad practice to run, for example, an SSH server to do maintenance work, as explained in~\textcite{noSSHDinContainers}. Most actions that would require having an SSH server running can also be achieved using Kubernetes features.

\paragraph{Secrets and Configuration}

Secrets and configuration items should not be baked into the container images, but should instead be managed using the Kubernetes objects \mycode{Secret} and \mycode{ConfigMap} respectively. These can then either be exposed to the containers using environment variables or by mounting them to a volume accessible to the container. By doing so, they can be more easily adapted and are not prone to leak when a container image leaks.

\paragraph{Rule-based Execution}

As explained in the~\textcite{k8sdocs}\footnote{\url{https://kubernetes.io/docs/concepts/policy/pod-security-policy/}, accessed 2019-06-25}, a \mycode{PodSecurityPolicy} can be used to enforce several rule-based execution restrictions. The configuration options include:

\begin{itemize}
    \item \mycode{seccomp}, as documented in~\textcite{seccompMan}, can limit the allowed system calls for user-space applications. At the time of this writing, \mycode{seccomp} is disabled by default in Kubernetes so that even the default Docker profile\footnote{\url{https://docs.docker.com/engine/security/seccomp/}, accessed 2019-06-28} does not apply.
    \item Policies of SELinux\footnote{\url{https://selinuxproject.org/page/Main_Page}, accessed 2019-06-25} and AppArmor\footnote{\url{https://kubernetes.io/docs/tutorials/clusters/apparmor/}, accessed 2019-06-25} can be configured for finer-grained access control. 
    \item Linux capabilities, as explained in~\textcite{linuxCaps}, are a kernel feature that can grant a process running as root some, but not all privileged capabilities. In Kubernetes, the whitelist of capabilities that can be added to a container can be configured.
\end{itemize}

%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables: 
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End: 
